{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48219459",
   "metadata": {},
   "source": [
    "# Data Profiling, Cleaning & EDA-Sierraleone\n",
    "**Objective:** Profile, clean, and explore the solar dataset for Sierraleone so it’s ready for comparison and region-ranking tasks.\n",
    "\n",
    "This notebook includes:\n",
    "- Summary statistics and missing-value report\n",
    "- Outlier detection and cleaning\n",
    "- Time series analysis\n",
    "- Correlation and scatter plots\n",
    "- Wind and temperature analysis\n",
    "- Bubble charts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579bc21d",
   "metadata": {},
   "source": [
    "## Importing the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667d6946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f557bb8",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9005f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set both plotting and display settings\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r\"D:\\Python\\Week_01\\data\\data\\sierraleone-bumbuna.csv\")\n",
    "\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb55f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show basic info\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f22707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Display the first 5 rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b55d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last 10 rows\n",
    "print(\"\\nLast 10 rows:\")\n",
    "display(df.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093e86d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 10 random sample rows\n",
    "print(\"\\nRandom sample of 10 rows:\")\n",
    "display(df.sample(10, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a754ab8",
   "metadata": {},
   "source": [
    "## Summary statistics and missing-value report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf16e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numeric columns\n",
    "print(\"\\nSummary statistics for numeric columns:\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c27899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# Percentage of missing values per column\n",
    "null_percent = df.isna().mean() * 100\n",
    "print(\"\\nPercentage of missing values:\")\n",
    "print((null_percent).round(2))\n",
    "\n",
    "# Filter columns with more than 5% nulls\n",
    "cols_with_nulls = null_percent[null_percent > 5].index.tolist()\n",
    "print(\"\\nColumns with >5% nulls:\", cols_with_nulls)\n",
    "\n",
    "# Exact duplicate rows\n",
    "dup_count = df.duplicated().sum()\n",
    "print(\"Duplicate rows:\", dup_count)\n",
    "\n",
    "# Cardinality (uniqueness) for categoricals\n",
    "cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "cardinality = {c: df[c].nunique() for c in cat_cols}\n",
    "print(\"Cardinality (categoricals):\", cardinality)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce311e6",
   "metadata": {},
   "source": [
    "## Interpretation of Summary statistics and missing-value report\n",
    "\n",
    "### 1. **General Overview**\n",
    "- The dataset has 525,600 records — suggests 60 min × 24 hr × 365 days = 1 year of minute-level data.\n",
    "- No missing numerical data (count = 525,600 for all measured variables).\n",
    "- Comments column is empty (count = 0); can be dropped\n",
    "- **Solar data (GHI, DNI, DHI)**: Negative GHI/DNI/DHI values are incorrect entries or sensor noise need correction\n",
    "- **Module data (ModA, ModB)**: Consistent with irradiance\n",
    "- **Temperature (Tamb, TModA, TModB)**: Physically valid\n",
    "- **Humidity (RH)**: Reasonable; 9.9 %(min) low outlier may indicate a dry period or sensor drift.\n",
    "- **Wind (WS, WSgust, WSstdev, WD, WDstdev)**: Wind readings are consistent; no clear data errors.\n",
    "- **Pressure (BP)**: Normal atmospheric range at moderate altitude\n",
    "- **Flags(Cleaning Flag & Precipitation)**: Sparse cleaning events → panels mostly uncleaned., no issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5233cce9",
   "metadata": {},
   "source": [
    "## Univariate Analysis for Numeric Columns Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e148767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for missing values, outliers, incorrect entries\n",
    "cols_radiation = ['GHI', 'DNI', 'DHI']\n",
    "cols_sensor = ['ModA', 'ModB']\n",
    "cols_wind = ['WS', 'WSgust']\n",
    "cols_misc = ['Cleaning', 'Precipitation']\n",
    "\n",
    "all_cols = cols_radiation + cols_sensor + cols_wind + cols_misc\n",
    "# Function to detect outliers using IQR\n",
    "def detect_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    return df[(df[column] < lower) | (df[column] > upper)]\n",
    "\n",
    "# Outliers summary\n",
    "print(\"\\nNumber of outliers per column:\")\n",
    "for col in all_cols:\n",
    "    outliers = detect_outliers(df, col)\n",
    "    print(f\"{col}: {len(outliers)}\")\n",
    "\n",
    "# Flag incorrect entries\n",
    "df_flags = pd.DataFrame(index=df.index)\n",
    "df_flags['Negative_Radiation'] = (df[cols_radiation] < 0).any(axis=1)\n",
    "df_flags['Negative_Wind'] = (df[cols_wind] < 0).any(axis=1)\n",
    "df_flags['Invalid_Cleaning'] = ~df['Cleaning'].isin([0, 1])\n",
    "df_flags['Negative_Precipitation'] = df['Precipitation'] < 0\n",
    "\n",
    "print(\"\\nRows with flagged incorrect entries:\")\n",
    "print(df_flags[df_flags.any(axis=1)])\n",
    "\n",
    "# Compute Z-scores and flag extreme values |Z|>3\n",
    "\n",
    "cols_zscore = cols_radiation + cols_sensor + cols_wind\n",
    "df_zscores = df[cols_zscore].apply(zscore)\n",
    "\n",
    "# Flag extreme values\n",
    "extreme_flags = (np.abs(df_zscores) > 3)\n",
    "print(\"\\nNumber of extreme Z-score values per column:\")\n",
    "print(extreme_flags.sum())\n",
    "\n",
    "# view rows with any extreme Z-score\n",
    "extreme_rows = df[extreme_flags.any(axis=1)]\n",
    "print(\"\\nRows with extreme Z-scores (|Z|>3):\")\n",
    "print(extreme_rows)\n",
    "\n",
    "# Handle missing values\n",
    "# Option 1: Drop rows with missing values in key columns\n",
    "# df_cleaned = df.dropna(subset=cols_radiation + cols_sensor + cols_wind)\n",
    "\n",
    "# Option 2: Impute missing values using median\n",
    "df_imputed = df.copy()\n",
    "for col in cols_radiation + cols_sensor + cols_wind + ['Precipitation']:\n",
    "    median_value = df_imputed[col].median()\n",
    "    #df_imputed[col].fillna(median_value, inplace=True)\n",
    "    df_imputed[col] = df_imputed[col].fillna(median_value)\n",
    "\n",
    "# Verify missing values are handled\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df_imputed[all_cols].isna().sum())\n",
    "# visualize distributions and outliers\n",
    "# ---------------------------\n",
    "\n",
    "for col in all_cols:\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Histogram on the left\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(df_imputed[col], bins=50, kde=True, color='skyblue')\n",
    "    plt.title(f'Histogram of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Boxplot on the right\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(y=df_imputed[col], color='lightgreen')\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.ylabel(col)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11c656",
   "metadata": {},
   "source": [
    "### Interpretation of Box plot and Histogram Report\n",
    "1. GHI, DNI, DHI (Radiation Columns)\n",
    "Histogram Interpretation:\n",
    "•\tUsually right-skewed because there are many low values (nighttime) and fewer high values (midday).\n",
    "•\tPeaks around solar noon if data is from daytime.\n",
    "•\tAny negative values would be physically impossible → indicate sensor error.\n",
    "Boxplot Interpretation:\n",
    "•\tMedian near the central value of daytime radiation.\n",
    "•\tOutliers: extremely high spikes could indicate sensor glitches.\n",
    "•\tValues below 0 should be flagged.\n",
    "\n",
    "2. ModA, ModB (Sensor Readings)\n",
    "Histogram Interpretation:\n",
    "•\tOften roughly normal if sensors behave consistently.\n",
    "•\tPeaks indicate common operating ranges.\n",
    "•\tBimodal or irregular shapes can signal malfunction or calibration issues.\n",
    "Boxplot Interpretation:\n",
    "•\tOutliers: unusually high or low readings may indicate sensor errors.\n",
    "•\tCheck symmetry: large deviations on one side may suggest drift.\n",
    "________________________________________\n",
    "3. WS, WSgust (Wind Speed)\n",
    "Histogram Interpretation:\n",
    "•\tUsually right-skewed: most readings are low, occasional gusts are high.\n",
    "•\tNegative values are physically impossible → must be flagged.\n",
    "Boxplot Interpretation:\n",
    "•\tOutliers represent strong gusts.\n",
    "•\tMedian and quartiles help understand typical wind conditions.\n",
    "•\tIf the box is very narrow, the sensor may not be capturing variability well.\n",
    "________________________________________\n",
    "4. Cleaning (1 or 0)\n",
    "Histogram Interpretation:\n",
    "•\tOnly two bars at 0 and 1.\n",
    "•\tShows frequency of cleaning events.\n",
    "Boxplot Interpretation:\n",
    "•\tWith only two unique values, boxplot is not very informative.\n",
    "•\tAny values other than 0 or 1 are invalid → need correction.\n",
    "________________________________________\n",
    "5. Precipitation (mm/min)\n",
    "Histogram Interpretation:\n",
    "•\tHighly right-skewed: most minutes have no rain (0), occasional high rainfall minutes create a long tail.\n",
    "•\tNegative values are impossible → indicate errors.\n",
    "Boxplot Interpretation:\n",
    "•\tOutliers correspond to intense rain events.\n",
    "•\tMedian is likely 0 or very low, reflecting mostly dry periods.\n",
    "________________________________________\n",
    "Summary of What to Look For\n",
    "1.\tFrom Histograms:\n",
    "o\tDistribution shape → normal, skewed, bimodal\n",
    "o\tPeaks → typical values\n",
    "o\tImpossible values (negative for radiation, wind, precipitation)\n",
    "2.\tFrom Boxplots:\n",
    "o\tOutliers → unusually high or low values\n",
    "o\tMedian & quartiles → typical operating range\n",
    "o\tFlags potential sensor errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e272b58",
   "metadata": {},
   "source": [
    "## Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbf50b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "# Define relevant columns for cleaning\n",
    "cols_radiation = ['GHI', 'DNI', 'DHI']\n",
    "cols_sensor = ['ModA', 'ModB']\n",
    "cols_wind = ['WS', 'WSgust']\n",
    "cols_misc = ['Cleaning', 'Precipitation']\n",
    "\n",
    "cols_numeric_for_impute = cols_radiation + cols_sensor + cols_wind + ['Precipitation']\n",
    "cols_for_zscore = cols_radiation + cols_sensor + cols_wind\n",
    "\n",
    "# ---------------------------\n",
    "#  Handle missing values: Impute median for key numeric columns\n",
    "# ---------------------------\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "for col in cols_numeric_for_impute:\n",
    "    median_value = df_cleaned[col].median()\n",
    "    df_cleaned[col] = df_cleaned[col].fillna(median_value)\n",
    "\n",
    "# For Cleaning, fill missing with 0 (assuming no cleaning event if missing)\n",
    "df_cleaned['Cleaning'] = df_cleaned['Cleaning'].fillna(0)\n",
    "\n",
    "# ---------------------------\n",
    "# Remove impossible values\n",
    "# ---------------------------\n",
    "# Negative values for radiation, wind, precipitation\n",
    "df_cleaned = df_cleaned[(df_cleaned[cols_radiation] >= 0).all(axis=1)]\n",
    "df_cleaned = df_cleaned[(df_cleaned[cols_wind] >= 0).all(axis=1)]\n",
    "df_cleaned = df_cleaned[df_cleaned['Precipitation'] >= 0]\n",
    "\n",
    "# Ensure Cleaning is only 0 or 1\n",
    "df_cleaned = df_cleaned[df_cleaned['Cleaning'].isin([0, 1])]\n",
    "\n",
    "# ---------------------------\n",
    "# Remove extreme outliers using Z-score (|Z|>3)\n",
    "# ---------------------------\n",
    "z_scores = df_cleaned[cols_for_zscore].apply(zscore)\n",
    "\n",
    "# Keep rows where all Z-scores are within ±3\n",
    "df_cleaned = df_cleaned[(np.abs(z_scores) <= 3).all(axis=1)]\n",
    "\n",
    "# ---------------------------\n",
    "# Export cleaned dataset (all columns included)\n",
    "# ---------------------------\n",
    "output_path = r\"D:\\Python\\Week_01\\Assignment\\solar-challenge-week0\\data\\sierralione_clean.csv\"\n",
    "df_cleaned.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset exported to: {output_path}\")\n",
    "print(f\"Original rows: {len(df)}, Cleaned rows: {len(df_cleaned)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0537a1",
   "metadata": {},
   "source": [
    "## Bivariate Analysis\n",
    "### Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f41196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1️⃣ Load the cleaned dataset\n",
    "# ---------------------------------------------\n",
    "df_clean = pd.read_csv(r\"D:\\Python\\Week_01\\Assignment\\solar-challenge-week0\\data\\sierralione_clean.csv\")\n",
    "\n",
    "# Ensure Timestamp column exists and convert to datetime\n",
    "df_clean.columns = df_clean.columns.str.strip()\n",
    "if 'Timestamp' not in df_clean.columns:\n",
    "    raise KeyError(\"Column 'Timestamp' not found in the dataset.\")\n",
    "\n",
    "df_clean['Timestamp'] = pd.to_datetime(df_clean['Timestamp'], errors='coerce')\n",
    "df_clean = df_clean.dropna(subset=['Timestamp'])\n",
    "df_clean = df_clean.sort_values('Timestamp')\n",
    "\n",
    "# Extract Date for daily bar aggregation\n",
    "df_clean['Date'] = df_clean['Timestamp'].dt.date\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2️⃣ Define Variables for Visualization\n",
    "# ---------------------------------------------\n",
    "variables = ['GHI', 'DNI', 'DHI', 'Tamb']\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3️⃣ Generate Line and Bar Charts Separately\n",
    "# ---------------------------------------------\n",
    "for var in variables:\n",
    "    if var not in df_clean.columns:\n",
    "        print(f\"⚠️ Skipping {var} — column not found.\")\n",
    "        continue\n",
    "\n",
    "    # ---- Line Chart ----\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.plot(df_clean['Timestamp'], df_clean[var], color='tab:blue', linewidth=1)\n",
    "    plt.title(f'{var} vs Timestamp (Line Chart)')\n",
    "    plt.xlabel('Timestamp')\n",
    "    plt.ylabel(var)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # ---- Daily Average Bar Chart ----\n",
    "    daily_avg = df_clean.groupby('Date')[var].mean().reset_index()\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.bar(daily_avg['Date'], daily_avg[var], color='skyblue')\n",
    "    plt.title(f'Daily Average {var} vs Date (Bar Chart)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(f'Average {var}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0a2e8a",
   "metadata": {},
   "source": [
    "## Time Series Interpretation \n",
    "1. GHI (Global Horizontal Irradiance)\n",
    "- GHI shows strong diurnal variation, with clear peaks on sunny days and dips during cloudy or rainy periods. \n",
    "- These variations align with normal solar behavior, indicating that data collection is generally consistent.\n",
    "________________________________________\n",
    "2. DNI (Direct Normal Irradiance)\n",
    "- The DNI time series exhibits sharp peaks on clear-sky days, confirming normal direct radiation patterns. \n",
    "- Periods with lower or fluctuating DNI indicate transient cloud cover or atmospheric scattering.\n",
    "________________________________________\n",
    "3. DHI (Diffuse Horizontal Irradiance)\n",
    "- DHI values tend to rise when DNI decreases — a typical inverse relationship. \n",
    "- This confirms expected physical behavior between diffuse and direct components of solar radiation.\n",
    "________________________________________\n",
    "4. Tamb (Ambient Temperature)\n",
    "- Tamb correlates positively with solar irradiance — higher temperatures occur during high GHI/DNI periods, confirming physical consistency. - Minor lags are expected and indicate correct temperature dynamics.\n",
    "________________________________________\n",
    "5. Interpretation of the Daily Average Bar Charts\n",
    "- The daily bar charts reveal overall solar resource variability and weather trends over time. \n",
    "- Periods with low GHI/DNI and high DHI suggest cloudy or rainy weather, while consistent high values imply clear-sky periods.\n",
    "\n",
    "\n",
    "6. Recommendations\n",
    "\n",
    "- Data Consistency: The smooth daily patterns suggest reliable sensors, but further checks for missing nighttime or constant values are advised.\n",
    "\n",
    "- Outlier Review: Extreme values or flat lines should be validated against maintenance logs.\n",
    "\n",
    "- Model Input: Cleaned GHI, DNI, and DHI data can be used to model solar PV or thermal system performance.\n",
    "\n",
    "- Temporal Aggregation: Hourly or daily means can be used for forecasting and trend analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8703dee0",
   "metadata": {},
   "source": [
    "### Pattern Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c518efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -----------------------------------------\n",
    "# 1️⃣ Load the cleaned dataset\n",
    "# -----------------------------------------\n",
    "df_clean = pd.read_csv(r\"D:\\Python\\Week_01\\Assignment\\solar-challenge-week0\\data\\sierralione_clean.csv\")\n",
    "\n",
    "# Ensure Timestamp column exists and convert to datetime\n",
    "df_clean.columns = df_clean.columns.str.strip()\n",
    "if 'Timestamp' not in df_clean.columns:\n",
    "    raise KeyError(\"Column 'Timestamp' not found in the dataset.\")\n",
    "\n",
    "df_clean['Timestamp'] = pd.to_datetime(df_clean['Timestamp'], errors='coerce')\n",
    "df_clean = df_clean.dropna(subset=['Timestamp'])\n",
    "df_clean = df_clean.sort_values('Timestamp')\n",
    "\n",
    "# Extract useful time components\n",
    "df_clean['Month'] = df_clean['Timestamp'].dt.month\n",
    "df_clean['Hour'] = df_clean['Timestamp'].dt.hour\n",
    "df_clean['Date'] = df_clean['Timestamp'].dt.date\n",
    "\n",
    "# -----------------------------------------\n",
    "# 2️⃣ Monthly Patterns (GHI, DNI, DHI, Tamb)\n",
    "# -----------------------------------------\n",
    "monthly_avg = df_clean.groupby('Month')[['GHI', 'DNI', 'DHI', 'Tamb']].mean()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "monthly_avg.plot(kind='bar', figsize=(12, 6), colormap='plasma')\n",
    "plt.title('Monthly Average Patterns of Solar Irradiance and Temperature')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Value')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------\n",
    "# 3️⃣ Daily (Hourly) Trends\n",
    "# -----------------------------------------\n",
    "hourly_avg = df_clean.groupby('Hour')[['GHI', 'DNI', 'DHI', 'Tamb']].mean()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "hourly_avg.plot(figsize=(12, 6), linewidth=2, colormap='viridis')\n",
    "plt.title('Average Hourly Trends of Solar Irradiance and Temperature')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Average Value')\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------------------\n",
    "# 4️⃣ Detect and Visualize Anomalies\n",
    "# -----------------------------------------\n",
    "for col in ['GHI', 'DNI', 'DHI', 'Tamb']:\n",
    "    if col in df_clean.columns:\n",
    "        threshold = df_clean[col].mean() + 3 * df_clean[col].std()\n",
    "        df_clean[f'{col}_anomaly'] = df_clean[col] > threshold\n",
    "        count = df_clean[f'{col}_anomaly'].sum()\n",
    "        print(f\"⚠️ {col}: {count} potential anomalies detected (values > mean + 3*std)\")\n",
    "\n",
    "# Example: Visualize anomalies for GHI\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(df_clean['Timestamp'], df_clean['GHI'], label='GHI', color='orange')\n",
    "plt.scatter(\n",
    "    df_clean.loc[df_clean['GHI_anomaly'], 'Timestamp'],\n",
    "    df_clean.loc[df_clean['GHI_anomaly'], 'GHI'],\n",
    "    color='red', label='Anomalies', s=20\n",
    ")\n",
    "plt.title('GHI Time Series with Anomalies Highlighted')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('GHI')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bda2c8e",
   "metadata": {},
   "source": [
    "### Time Series Interpretation- Monthly and Daily\n",
    "1. Monthly Patterns\n",
    "- The monthly variation in GHI and DNI confirms the site’s dependence on seasonal weather patterns — clear skies increase direct irradiance while cloud cover reduces it.\n",
    "- DHI’s opposite trend serves as an indirect indicator of atmospheric conditions.\n",
    "- The temperature trend correlates well with solar exposure, reflecting expected climatic variation in the Bumbuna region.\n",
    "________________________________________\n",
    "2. Daily (Diurnal) Trends\n",
    "- These diurnal patterns are physically consistent with natural solar behavior. \n",
    "- The lag between irradiance and temperature highlights the thermal inertia of the environment — the atmosphere and surfaces take time to warm and cool.\n",
    "- Any deviation from these smooth patterns (e.g., sudden spikes, missing peaks, or flattened curves) may indicate sensor errors, maintenance events, or transient weather phenomena such as storms or shading.\n",
    "________________________________________\n",
    "3. Anomalies and Unusual Observations\n",
    "- **Detected Anomalies**\n",
    "     - Spikes in GHI/DNI during early morning or late evening hours may indicate sensor misalignment or reflection.\n",
    "     - Sudden drops to zero during daylight hours may correspond to cloud passages, dust, or temporary obstructions.\n",
    "     - Temperature outliers (sharp increases or decreases) could result from sensor calibration drift, rain cooling, or abrupt environmental changes.\n",
    "     - Flat-line readings (constant values over extended periods) may suggest sensor malfunction or data logging errors.\n",
    "- **Interpretation**\n",
    "- Identifying and investigating these anomalies is crucial for ensuring the quality of solar datasets.\n",
    "- Persistent outliers should be compared against weather logs and maintenance records. Data gaps or unrealistic values can be corrected through imputation or flagged for exclusion before modeling or system design.\n",
    "\n",
    "**Conclusion**\n",
    "- The temporal analysis of the Sierra Leone Bumbuna dataset reveals strong seasonal and diurnal consistency, confirming that the sensors capture realistic solar and meteorological patterns.\n",
    "- Observed anomalies are within expected operational ranges and can be managed with standard cleaning procedures (e.g., Z-score filtering, interpolation, or outlier masking).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e112a1bd",
   "metadata": {},
   "source": [
    "### Showing Cleaning Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb7bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip column names to avoid whitespace issues\n",
    "df_clean.columns = df_clean.columns.str.strip()\n",
    "\n",
    "# Check required columns\n",
    "required_cols = ['Cleaning', 'ModA', 'ModB']\n",
    "for col in required_cols:\n",
    "    if col not in df_clean.columns:\n",
    "        raise KeyError(f\"Column '{col}' not found in dataset.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Group by Cleaning flag and compute averages\n",
    "# -----------------------------\n",
    "avg_mod = df_clean.groupby('Cleaning')[['ModA', 'ModB']].mean().reset_index()\n",
    "avg_mod['Cleaning'] = avg_mod['Cleaning'].map({0: 'Pre-Clean', 1: 'Post-Clean'})  # Optional: label\n",
    "\n",
    "print(\"Average ModA & ModB pre- and post-clean:\")\n",
    "print(avg_mod)\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Plot the results\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x='Cleaning', y='value', hue='variable',\n",
    "            data=pd.melt(avg_mod, id_vars='Cleaning', value_vars=['ModA', 'ModB']),\n",
    "            palette='Set2')\n",
    "\n",
    "plt.title('Average ModA & ModB Pre- and Post-Clean')\n",
    "plt.ylabel('Average Value')\n",
    "plt.xlabel('Cleaning Status')\n",
    "plt.legend(title='Module')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d869d60",
   "metadata": {},
   "source": [
    "## Interpretation of the Cleaning Impact\n",
    "1. Observation from the bar chart and grouped averages:\n",
    "\n",
    "- Both ModA and ModB show higher average readings after cleaning (Post-Clean) compared to before (Pre-Clean).\n",
    "\n",
    "- The increase is consistent across both modules, suggesting that the cleaning process improved sensor performance or removed obstructions affecting measurements.\n",
    "\n",
    "- The difference between ModA and ModB is relatively small, indicating that both sensors respond similarly to cleaning.\n",
    "\n",
    "2. Implication:\n",
    "\n",
    "- The cleaning process effectively restored the accuracy and reliability of the module readings.\n",
    "\n",
    "- Pre-clean data may have been underreporting irradiance or module output due to dirt, dust, or other obstructions.\n",
    "\n",
    "3. Conclusion\n",
    "\n",
    "- Cleaning has a measurable positive impact on module sensor performance.\n",
    "\n",
    "- Post-cleaning readings are more representative of actual solar irradiance or module output.\n",
    "\n",
    "- This confirms that regular maintenance and cleaning are essential for reliable solar monitoring and performance evaluation.\n",
    "\n",
    "The analysis is a bivariate study examining one categorical factor (Cleaning) against numerical outcomes (ModA and ModB).\n",
    "\n",
    "4. Recommendations\n",
    "\n",
    "- Regular Cleaning Schedule\n",
    "\n",
    "    - Implement a routine cleaning protocol for all solar modules to maintain optimal sensor performance.\n",
    "\n",
    "    - Frequency should be determined based on dust levels, rainfall, and module sensitivity.\n",
    "\n",
    "- Monitor Module Performance Post-Cleaning\n",
    "\n",
    "    - Continue to track ModA and ModB outputs to confirm the effectiveness of each cleaning cycle.\n",
    "\n",
    "    - Use automated alerts for unexpected drops in readings that may indicate dirt accumulation or sensor malfunction.\n",
    "\n",
    "- Data Quality Management\n",
    "\n",
    "    - Flag pre-clean readings in historical datasets when performing analysis to account for potential underreporting.\n",
    "\n",
    "    - Consider imputation or adjustment for pre-clean values in performance modeling.\n",
    "\n",
    "- Extend Analysis to Other Modules\n",
    "\n",
    "    - If the facility has more modules, apply the same cleaning-effect analysis to all to ensure consistency across the system.\n",
    "\n",
    "- Integrate Cleaning Impact into Predictive Models\n",
    "\n",
    "    - Factor the cleaning effect into solar energy production forecasts to improve accuracy.\n",
    "\n",
    "**Strategic Takeaway**\n",
    "Maintaining module cleanliness is a simple yet impactful operational strategy that directly improves sensor accuracy and overall energy yield predictions. Regular cleaning combined with monitoring ensures data reliability, better performance analysis, and informed investment decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5589b21",
   "metadata": {},
   "source": [
    "## Multivariant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861736bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from windrose import WindroseAxes\n",
    "\n",
    "# -----------------------------\n",
    "# 1️⃣ Load cleaned dataset\n",
    "# -----------------------------\n",
    "df_clean = pd.read_csv(r\"D:\\Python\\Week_01\\Assignment\\solar-challenge-week0\\data\\sierralione_clean.csv\")\n",
    "df_clean.columns = df_clean.columns.str.strip()  # Remove leading/trailing spaces\n",
    "\n",
    "# -----------------------------\n",
    "# 2️⃣ Correlation Heatmap\n",
    "# -----------------------------\n",
    "corr_vars = ['GHI', 'DNI', 'DHI', 'TModA', 'TModB']\n",
    "corr_matrix = df_clean[corr_vars].corr()\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True)\n",
    "plt.title('Correlation Heatmap: GHI, DNI, DHI, TModA, TModB')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 3️⃣ Scatter Plots\n",
    "# -----------------------------\n",
    "scatter_vars = ['WS', 'WSgust', 'WD']\n",
    "for var in scatter_vars:\n",
    "    if var in df_clean.columns:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.scatterplot(x=df_clean[var], y=df_clean['GHI'])\n",
    "        plt.title(f'{var} vs GHI')\n",
    "        plt.xlabel(var)\n",
    "        plt.ylabel('GHI')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# RH vs Tamb and RH vs GHI\n",
    "for y_var in ['Tamb', 'GHI']:\n",
    "    if 'RH' in df_clean.columns:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        sns.scatterplot(x=df_clean['RH'], y=df_clean[y_var])\n",
    "        plt.title(f'RH vs {y_var}')\n",
    "        plt.xlabel('Relative Humidity (RH)')\n",
    "        plt.ylabel(y_var)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 4️⃣ Bubble Chart: GHI vs Tamb, bubble size = RH or BP\n",
    "# -----------------------------\n",
    "if 'RH' in df_clean.columns:\n",
    "    size_var = 'RH'\n",
    "elif 'BP' in df_clean.columns:\n",
    "    size_var = 'BP'\n",
    "else:\n",
    "    size_var = None\n",
    "\n",
    "if size_var:\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(df_clean['GHI'], df_clean['Tamb'], \n",
    "                s=df_clean[size_var]*0.5, alpha=0.5, c=df_clean[size_var], cmap='viridis')\n",
    "    plt.colorbar(label=size_var)\n",
    "    plt.xlabel('GHI')\n",
    "    plt.ylabel('Tamb')\n",
    "    plt.title(f'Bubble Chart: GHI vs Tamb, Bubble size = {size_var}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# 5️⃣ Wind Rose: WS and WD\n",
    "# -----------------------------\n",
    "if 'WS' in df_clean.columns and 'WD' in df_clean.columns:\n",
    "    plt.figure(figsize=(8,8))\n",
    "    ax = WindroseAxes.from_ax()\n",
    "    ax.bar(df_clean['WD'], df_clean['WS'], normed=True, opening=0.8, edgecolor='white')\n",
    "    ax.set_legend(title='Wind Speed (WS)')\n",
    "    plt.title('Wind Rose: WS vs WD')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
