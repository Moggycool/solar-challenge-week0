{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48219459",
   "metadata": {},
   "source": [
    "# Data Profiling, Cleaning & EDA-Sierraleone\n",
    "**Objective:** Profile, clean, and explore the solar dataset for Sierraleone so it’s ready for comparison and region-ranking tasks.\n",
    "\n",
    "This notebook includes:\n",
    "- Summary statistics and missing-value report\n",
    "- Outlier detection and cleaning\n",
    "- Time series analysis\n",
    "- Correlation and scatter plots\n",
    "- Wind and temperature analysis\n",
    "- Bubble charts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579bc21d",
   "metadata": {},
   "source": [
    "## Importing the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667d6946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f557bb8",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9005f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set both plotting and display settings\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r\"D:\\Python\\Week_01\\data\\data\\sierraleone-bumbuna.csv\")\n",
    "\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb55f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show basic info\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f22707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Display the first 5 rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b55d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last 10 rows\n",
    "print(\"\\nLast 10 rows:\")\n",
    "display(df.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093e86d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 10 random sample rows\n",
    "print(\"\\nRandom sample of 10 rows:\")\n",
    "display(df.sample(10, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a754ab8",
   "metadata": {},
   "source": [
    "## Summary statistics and missing-value report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf16e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numeric columns\n",
    "print(\"\\nSummary statistics for numeric columns:\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c27899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# Percentage of missing values per column\n",
    "null_percent = df.isna().mean() * 100\n",
    "print(\"\\nPercentage of missing values:\")\n",
    "print((null_percent).round(2))\n",
    "\n",
    "# Filter columns with more than 5% nulls\n",
    "cols_with_nulls = null_percent[null_percent > 5].index.tolist()\n",
    "print(\"\\nColumns with >5% nulls:\", cols_with_nulls)\n",
    "\n",
    "# Exact duplicate rows\n",
    "dup_count = df.duplicated().sum()\n",
    "print(\"Duplicate rows:\", dup_count)\n",
    "\n",
    "# Cardinality (uniqueness) for categoricals\n",
    "cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "cardinality = {c: df[c].nunique() for c in cat_cols}\n",
    "print(\"Cardinality (categoricals):\", cardinality)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce311e6",
   "metadata": {},
   "source": [
    "## Interpretation of Summary statistics and missing-value report\n",
    "\n",
    "### 1. **General Overview**\n",
    "- The dataset has 525,600 records — suggests 60 min × 24 hr × 365 days = 1 year of minute-level data.\n",
    "- No missing numerical data (count = 525,600 for all measured variables).\n",
    "- Comments column is empty (count = 0); can be dropped\n",
    "- **Solar data (GHI, DNI, DHI)**: Negative GHI/DNI/DHI values are incorrect entries or sensor noise need correction\n",
    "- **Module data (ModA, ModB)**: Consistent with irradiance\n",
    "- **Temperature (Tamb, TModA, TModB)**: Physically valid\n",
    "- **Humidity (RH)**: Reasonable; 9.9 %(min) low outlier may indicate a dry period or sensor drift.\n",
    "- **Wind (WS, WSgust, WSstdev, WD, WDstdev)**: Wind readings are consistent; no clear data errors.\n",
    "- **Pressure (BP)**: Normal atmospheric range at moderate altitude\n",
    "- **Flags(Cleaning Flag & Precipitation)**: Sparse cleaning events → panels mostly uncleaned., no issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5233cce9",
   "metadata": {},
   "source": [
    "## Univariate Analysis for Numeric Columns Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e148767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for missing values, outliers, incorrect entries\n",
    "cols_radiation = ['GHI', 'DNI', 'DHI']\n",
    "cols_sensor = ['ModA', 'ModB']\n",
    "cols_wind = ['WS', 'WSgust']\n",
    "cols_misc = ['Cleaning', 'Precipitation']\n",
    "\n",
    "all_cols = cols_radiation + cols_sensor + cols_wind + cols_misc\n",
    "# Function to detect outliers using IQR\n",
    "def detect_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    return df[(df[column] < lower) | (df[column] > upper)]\n",
    "\n",
    "# Outliers summary\n",
    "print(\"\\nNumber of outliers per column:\")\n",
    "for col in all_cols:\n",
    "    outliers = detect_outliers(df, col)\n",
    "    print(f\"{col}: {len(outliers)}\")\n",
    "\n",
    "# Flag incorrect entries\n",
    "df_flags = pd.DataFrame(index=df.index)\n",
    "df_flags['Negative_Radiation'] = (df[cols_radiation] < 0).any(axis=1)\n",
    "df_flags['Negative_Wind'] = (df[cols_wind] < 0).any(axis=1)\n",
    "df_flags['Invalid_Cleaning'] = ~df['Cleaning'].isin([0, 1])\n",
    "df_flags['Negative_Precipitation'] = df['Precipitation'] < 0\n",
    "\n",
    "print(\"\\nRows with flagged incorrect entries:\")\n",
    "print(df_flags[df_flags.any(axis=1)])\n",
    "\n",
    "# Compute Z-scores and flag extreme values |Z|>3\n",
    "\n",
    "cols_zscore = cols_radiation + cols_sensor + cols_wind\n",
    "df_zscores = df[cols_zscore].apply(zscore)\n",
    "\n",
    "# Flag extreme values\n",
    "extreme_flags = (np.abs(df_zscores) > 3)\n",
    "print(\"\\nNumber of extreme Z-score values per column:\")\n",
    "print(extreme_flags.sum())\n",
    "\n",
    "# view rows with any extreme Z-score\n",
    "extreme_rows = df[extreme_flags.any(axis=1)]\n",
    "print(\"\\nRows with extreme Z-scores (|Z|>3):\")\n",
    "print(extreme_rows)\n",
    "\n",
    "# Handle missing values\n",
    "# Option 1: Drop rows with missing values in key columns\n",
    "# df_cleaned = df.dropna(subset=cols_radiation + cols_sensor + cols_wind)\n",
    "\n",
    "# Option 2: Impute missing values using median\n",
    "df_imputed = df.copy()\n",
    "for col in cols_radiation + cols_sensor + cols_wind + ['Precipitation']:\n",
    "    median_value = df_imputed[col].median()\n",
    "    #df_imputed[col].fillna(median_value, inplace=True)\n",
    "    df_imputed[col] = df_imputed[col].fillna(median_value)\n",
    "\n",
    "# Verify missing values are handled\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df_imputed[all_cols].isna().sum())\n",
    "# visualize distributions and outliers\n",
    "# ---------------------------\n",
    "\n",
    "for col in all_cols:\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Histogram on the left\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(df_imputed[col], bins=50, kde=True, color='skyblue')\n",
    "    plt.title(f'Histogram of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Boxplot on the right\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(y=df_imputed[col], color='lightgreen')\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.ylabel(col)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11c656",
   "metadata": {},
   "source": [
    "### Interpretation of Box plot and Histogram Report\n",
    "1. GHI, DNI, DHI (Radiation Columns)\n",
    "Histogram Interpretation:\n",
    "•\tUsually right-skewed because there are many low values (nighttime) and fewer high values (midday).\n",
    "•\tPeaks around solar noon if data is from daytime.\n",
    "•\tAny negative values would be physically impossible → indicate sensor error.\n",
    "Boxplot Interpretation:\n",
    "•\tMedian near the central value of daytime radiation.\n",
    "•\tOutliers: extremely high spikes could indicate sensor glitches.\n",
    "•\tValues below 0 should be flagged.\n",
    "\n",
    "2. ModA, ModB (Sensor Readings)\n",
    "Histogram Interpretation:\n",
    "•\tOften roughly normal if sensors behave consistently.\n",
    "•\tPeaks indicate common operating ranges.\n",
    "•\tBimodal or irregular shapes can signal malfunction or calibration issues.\n",
    "Boxplot Interpretation:\n",
    "•\tOutliers: unusually high or low readings may indicate sensor errors.\n",
    "•\tCheck symmetry: large deviations on one side may suggest drift.\n",
    "________________________________________\n",
    "3. WS, WSgust (Wind Speed)\n",
    "Histogram Interpretation:\n",
    "•\tUsually right-skewed: most readings are low, occasional gusts are high.\n",
    "•\tNegative values are physically impossible → must be flagged.\n",
    "Boxplot Interpretation:\n",
    "•\tOutliers represent strong gusts.\n",
    "•\tMedian and quartiles help understand typical wind conditions.\n",
    "•\tIf the box is very narrow, the sensor may not be capturing variability well.\n",
    "________________________________________\n",
    "4. Cleaning (1 or 0)\n",
    "Histogram Interpretation:\n",
    "•\tOnly two bars at 0 and 1.\n",
    "•\tShows frequency of cleaning events.\n",
    "Boxplot Interpretation:\n",
    "•\tWith only two unique values, boxplot is not very informative.\n",
    "•\tAny values other than 0 or 1 are invalid → need correction.\n",
    "________________________________________\n",
    "5. Precipitation (mm/min)\n",
    "Histogram Interpretation:\n",
    "•\tHighly right-skewed: most minutes have no rain (0), occasional high rainfall minutes create a long tail.\n",
    "•\tNegative values are impossible → indicate errors.\n",
    "Boxplot Interpretation:\n",
    "•\tOutliers correspond to intense rain events.\n",
    "•\tMedian is likely 0 or very low, reflecting mostly dry periods.\n",
    "________________________________________\n",
    "Summary of What to Look For\n",
    "1.\tFrom Histograms:\n",
    "o\tDistribution shape → normal, skewed, bimodal\n",
    "o\tPeaks → typical values\n",
    "o\tImpossible values (negative for radiation, wind, precipitation)\n",
    "2.\tFrom Boxplots:\n",
    "o\tOutliers → unusually high or low values\n",
    "o\tMedian & quartiles → typical operating range\n",
    "o\tFlags potential sensor errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e272b58",
   "metadata": {},
   "source": [
    "## Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbf50b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "# Define relevant columns for cleaning\n",
    "cols_radiation = ['GHI', 'DNI', 'DHI']\n",
    "cols_sensor = ['ModA', 'ModB']\n",
    "cols_wind = ['WS', 'WSgust']\n",
    "cols_misc = ['Cleaning', 'Precipitation']\n",
    "\n",
    "cols_numeric_for_impute = cols_radiation + cols_sensor + cols_wind + ['Precipitation']\n",
    "cols_for_zscore = cols_radiation + cols_sensor + cols_wind\n",
    "\n",
    "# ---------------------------\n",
    "#  Handle missing values: Impute median for key numeric columns\n",
    "# ---------------------------\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "for col in cols_numeric_for_impute:\n",
    "    median_value = df_cleaned[col].median()\n",
    "    df_cleaned[col] = df_cleaned[col].fillna(median_value)\n",
    "\n",
    "# For Cleaning, fill missing with 0 (assuming no cleaning event if missing)\n",
    "df_cleaned['Cleaning'] = df_cleaned['Cleaning'].fillna(0)\n",
    "\n",
    "# ---------------------------\n",
    "# Remove impossible values\n",
    "# ---------------------------\n",
    "# Negative values for radiation, wind, precipitation\n",
    "df_cleaned = df_cleaned[(df_cleaned[cols_radiation] >= 0).all(axis=1)]\n",
    "df_cleaned = df_cleaned[(df_cleaned[cols_wind] >= 0).all(axis=1)]\n",
    "df_cleaned = df_cleaned[df_cleaned['Precipitation'] >= 0]\n",
    "\n",
    "# Ensure Cleaning is only 0 or 1\n",
    "df_cleaned = df_cleaned[df_cleaned['Cleaning'].isin([0, 1])]\n",
    "\n",
    "# ---------------------------\n",
    "# Remove extreme outliers using Z-score (|Z|>3)\n",
    "# ---------------------------\n",
    "z_scores = df_cleaned[cols_for_zscore].apply(zscore)\n",
    "\n",
    "# Keep rows where all Z-scores are within ±3\n",
    "df_cleaned = df_cleaned[(np.abs(z_scores) <= 3).all(axis=1)]\n",
    "\n",
    "# ---------------------------\n",
    "# Export cleaned dataset (all columns included)\n",
    "# ---------------------------\n",
    "output_path = r\"D:\\Python\\Week_01\\Assignment\\solar-challenge-week0\\data\\sierralione_clean.csv\"\n",
    "df_cleaned.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset exported to: {output_path}\")\n",
    "print(f\"Original rows: {len(df)}, Cleaned rows: {len(df_cleaned)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0537a1",
   "metadata": {},
   "source": [
    "## Bivariate Analysis\n",
    "### Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f41196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1️⃣ Load the cleaned dataset\n",
    "# ---------------------------------------------\n",
    "df_clean = pd.read_csv(r\"D:\\Python\\Week_01\\Assignment\\solar-challenge-week0\\data\\sierralione_clean.csv\")\n",
    "\n",
    "# Ensure Timestamp column exists and convert to datetime\n",
    "df_clean.columns = df_clean.columns.str.strip()\n",
    "if 'Timestamp' not in df_clean.columns:\n",
    "    raise KeyError(\"Column 'Timestamp' not found in the dataset.\")\n",
    "\n",
    "df_clean['Timestamp'] = pd.to_datetime(df_clean['Timestamp'], errors='coerce')\n",
    "df_clean = df_clean.dropna(subset=['Timestamp'])\n",
    "df_clean = df_clean.sort_values('Timestamp')\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2️⃣ Line Chart: GHI, DNI, DHI, Tamb vs Timestamp\n",
    "# ---------------------------------------------\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(df_clean['Timestamp'], df_clean['GHI'], label='GHI', color='gold', linewidth=1)\n",
    "plt.plot(df_clean['Timestamp'], df_clean['DNI'], label='DNI', color='orange', linewidth=1)\n",
    "plt.plot(df_clean['Timestamp'], df_clean['DHI'], label='DHI', color='red', linewidth=1)\n",
    "plt.plot(df_clean['Timestamp'], df_clean['Tamb'], label='Tamb (°C)', color='blue', linewidth=1)\n",
    "\n",
    "plt.title('Time Series Line Chart of GHI, DNI, DHI, and Tamb')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Values')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3️⃣ Bar Chart: Aggregated Daily Averages\n",
    "# ---------------------------------------------\n",
    "# Compute daily means to reduce visual clutter\n",
    "df_clean['Date'] = df_clean['Timestamp'].dt.date\n",
    "daily_avg = df_clean.groupby('Date')[['GHI', 'DNI', 'DHI', 'Tamb']].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "daily_avg.plot(\n",
    "    x='Date',\n",
    "    y=['GHI', 'DNI', 'DHI', 'Tamb'],\n",
    "    kind='bar',\n",
    "    figsize=(14, 6),\n",
    "    colormap='viridis'\n",
    ")\n",
    "\n",
    "plt.title('Daily Average GHI, DNI, DHI, and Tamb (Bar Chart)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Value')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bda2c8e",
   "metadata": {},
   "source": [
    "## Time Series Interpretation \n",
    "1. GHI (Global Horizontal Irradiance)\n",
    "- GHI shows strong diurnal variation, with clear peaks on sunny days and dips during cloudy or rainy periods. \n",
    "- These variations align with normal solar behavior, indicating that data collection is generally consistent.\n",
    "________________________________________\n",
    "2. DNI (Direct Normal Irradiance)\n",
    "- The DNI time series exhibits sharp peaks on clear-sky days, confirming normal direct radiation patterns. \n",
    "- Periods with lower or fluctuating DNI indicate transient cloud cover or atmospheric scattering.\n",
    "________________________________________\n",
    "3. DHI (Diffuse Horizontal Irradiance)\n",
    "- DHI values tend to rise when DNI decreases — a typical inverse relationship. \n",
    "- This confirms expected physical behavior between diffuse and direct components of solar radiation.\n",
    "________________________________________\n",
    "4. Tamb (Ambient Temperature)\n",
    "- Tamb correlates positively with solar irradiance — higher temperatures occur during high GHI/DNI periods, confirming physical consistency. - Minor lags are expected and indicate correct temperature dynamics.\n",
    "________________________________________\n",
    "5. Interpretation of the Daily Average Bar Charts\n",
    "- The daily bar charts reveal overall solar resource variability and weather trends over time. \n",
    "- Periods with low GHI/DNI and high DHI suggest cloudy or rainy weather, while consistent high values imply clear-sky periods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb7bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
