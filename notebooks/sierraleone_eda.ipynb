{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48219459",
   "metadata": {},
   "source": [
    "# Data Profiling, Cleaning & EDA-Sierraleone\n",
    "**Objective:** Profile, clean, and explore the solar dataset for Sierraleone so it’s ready for comparison and region-ranking tasks.\n",
    "\n",
    "This notebook includes:\n",
    "- Summary statistics and missing-value report\n",
    "- Outlier detection and cleaning\n",
    "- Time series analysis\n",
    "- Correlation and scatter plots\n",
    "- Wind and temperature analysis\n",
    "- Bubble charts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579bc21d",
   "metadata": {},
   "source": [
    "## Importing the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667d6946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f557bb8",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9005f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set both plotting and display settings\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(r\"D:\\Python\\Week_01\\data\\data\\sierraleone-bumbuna.csv\")\n",
    "\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb55f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show basic info\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f22707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Display the first 5 rows\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b55d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last 10 rows\n",
    "print(\"\\nLast 10 rows:\")\n",
    "display(df.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093e86d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 10 random sample rows\n",
    "print(\"\\nRandom sample of 10 rows:\")\n",
    "display(df.sample(10, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a754ab8",
   "metadata": {},
   "source": [
    "## Summary statistics and missing-value report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf16e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numeric columns\n",
    "print(\"\\nSummary statistics for numeric columns:\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c27899d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# Percentage of missing values per column\n",
    "null_percent = df.isna().mean() * 100\n",
    "print(\"\\nPercentage of missing values:\")\n",
    "print((null_percent).round(2))\n",
    "\n",
    "# Filter columns with more than 5% nulls\n",
    "cols_with_nulls = null_percent[null_percent > 5].index.tolist()\n",
    "print(\"\\nColumns with >5% nulls:\", cols_with_nulls)\n",
    "\n",
    "# Exact duplicate rows\n",
    "dup_count = df.duplicated().sum()\n",
    "print(\"Duplicate rows:\", dup_count)\n",
    "\n",
    "# Cardinality (uniqueness) for categoricals\n",
    "cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "cardinality = {c: df[c].nunique() for c in cat_cols}\n",
    "print(\"Cardinality (categoricals):\", cardinality)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce311e6",
   "metadata": {},
   "source": [
    "## Interpretation of Summary statistics and missing-value report\n",
    "\n",
    "### 1. **General Overview**\n",
    "- The dataset has 525,600 records — suggests 60 min × 24 hr × 365 days = 1 year of minute-level data.\n",
    "- No missing numerical data (count = 525,600 for all measured variables).\n",
    "- Comments column is empty (count = 0); can be dropped\n",
    "- **Solar data (GHI, DNI, DHI)**: Negative GHI/DNI/DHI values are incorrect entries or sensor noise need correction\n",
    "- **Module data (ModA, ModB)**: Consistent with irradiance\n",
    "- **Temperature (Tamb, TModA, TModB)**: Physically valid\n",
    "- **Humidity (RH)**: Reasonable; 9.9 %(min) low outlier may indicate a dry period or sensor drift.\n",
    "- **Wind (WS, WSgust, WSstdev, WD, WDstdev)**: Wind readings are consistent; no clear data errors.\n",
    "- **Pressure (BP)**: Normal atmospheric range at moderate altitude\n",
    "- **Flags(Cleaning Flag & Precipitation)**: Sparse cleaning events → panels mostly uncleaned., no issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5233cce9",
   "metadata": {},
   "source": [
    "## Univariate Analysis for Numeric Columns Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e148767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for missing values, outliers, incorrect entries\n",
    "cols_radiation = ['GHI', 'DNI', 'DHI']\n",
    "cols_sensor = ['ModA', 'ModB']\n",
    "cols_wind = ['WS', 'WSgust']\n",
    "cols_misc = ['Cleaning', 'Precipitation']\n",
    "\n",
    "all_cols = cols_radiation + cols_sensor + cols_wind + cols_misc\n",
    "# Function to detect outliers using IQR\n",
    "def detect_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    return df[(df[column] < lower) | (df[column] > upper)]\n",
    "\n",
    "# Outliers summary\n",
    "print(\"\\nNumber of outliers per column:\")\n",
    "for col in all_cols:\n",
    "    outliers = detect_outliers(df, col)\n",
    "    print(f\"{col}: {len(outliers)}\")\n",
    "\n",
    "# Flag incorrect entries\n",
    "df_flags = pd.DataFrame(index=df.index)\n",
    "df_flags['Negative_Radiation'] = (df[cols_radiation] < 0).any(axis=1)\n",
    "df_flags['Negative_Wind'] = (df[cols_wind] < 0).any(axis=1)\n",
    "df_flags['Invalid_Cleaning'] = ~df['Cleaning'].isin([0, 1])\n",
    "df_flags['Negative_Precipitation'] = df['Precipitation'] < 0\n",
    "\n",
    "print(\"\\nRows with flagged incorrect entries:\")\n",
    "print(df_flags[df_flags.any(axis=1)])\n",
    "\n",
    "# Compute Z-scores and flag extreme values |Z|>3\n",
    "\n",
    "cols_zscore = cols_radiation + cols_sensor + cols_wind\n",
    "df_zscores = df[cols_zscore].apply(zscore)\n",
    "\n",
    "# Flag extreme values\n",
    "extreme_flags = (np.abs(df_zscores) > 3)\n",
    "print(\"\\nNumber of extreme Z-score values per column:\")\n",
    "print(extreme_flags.sum())\n",
    "\n",
    "# view rows with any extreme Z-score\n",
    "extreme_rows = df[extreme_flags.any(axis=1)]\n",
    "print(\"\\nRows with extreme Z-scores (|Z|>3):\")\n",
    "print(extreme_rows)\n",
    "\n",
    "# Handle missing values\n",
    "# Option 1: Drop rows with missing values in key columns\n",
    "# df_cleaned = df.dropna(subset=cols_radiation + cols_sensor + cols_wind)\n",
    "\n",
    "# Option 2: Impute missing values using median\n",
    "df_imputed = df.copy()\n",
    "for col in cols_radiation + cols_sensor + cols_wind + ['Precipitation']:\n",
    "    median_value = df_imputed[col].median()\n",
    "    #df_imputed[col].fillna(median_value, inplace=True)\n",
    "    df_imputed[col] = df_imputed[col].fillna(median_value)\n",
    "\n",
    "# Verify missing values are handled\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df_imputed[all_cols].isna().sum())\n",
    "# visualize distributions and outliers\n",
    "# ---------------------------\n",
    "\n",
    "for col in all_cols:\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Histogram on the left\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(df_imputed[col], bins=50, kde=True, color='skyblue')\n",
    "    plt.title(f'Histogram of {col}')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Boxplot on the right\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.boxplot(y=df_imputed[col], color='lightgreen')\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.ylabel(col)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11c656",
   "metadata": {},
   "source": [
    "### Interpretation of Box plot and Histogram Report\n",
    "1. GHI, DNI, DHI (Radiation Columns)\n",
    "Histogram Interpretation:\n",
    "•\tUsually right-skewed because there are many low values (nighttime) and fewer high values (midday).\n",
    "•\tPeaks around solar noon if data is from daytime.\n",
    "•\tAny negative values would be physically impossible → indicate sensor error.\n",
    "Boxplot Interpretation:\n",
    "•\tMedian near the central value of daytime radiation.\n",
    "•\tOutliers: extremely high spikes could indicate sensor glitches.\n",
    "•\tValues below 0 should be flagged.\n",
    "\n",
    "2. ModA, ModB (Sensor Readings)\n",
    "Histogram Interpretation:\n",
    "•\tOften roughly normal if sensors behave consistently.\n",
    "•\tPeaks indicate common operating ranges.\n",
    "•\tBimodal or irregular shapes can signal malfunction or calibration issues.\n",
    "Boxplot Interpretation:\n",
    "•\tOutliers: unusually high or low readings may indicate sensor errors.\n",
    "•\tCheck symmetry: large deviations on one side may suggest drift.\n",
    "________________________________________\n",
    "3. WS, WSgust (Wind Speed)\n",
    "Histogram Interpretation:\n",
    "•\tUsually right-skewed: most readings are low, occasional gusts are high.\n",
    "•\tNegative values are physically impossible → must be flagged.\n",
    "Boxplot Interpretation:\n",
    "•\tOutliers represent strong gusts.\n",
    "•\tMedian and quartiles help understand typical wind conditions.\n",
    "•\tIf the box is very narrow, the sensor may not be capturing variability well.\n",
    "________________________________________\n",
    "4. Cleaning (1 or 0)\n",
    "Histogram Interpretation:\n",
    "•\tOnly two bars at 0 and 1.\n",
    "•\tShows frequency of cleaning events.\n",
    "Boxplot Interpretation:\n",
    "•\tWith only two unique values, boxplot is not very informative.\n",
    "•\tAny values other than 0 or 1 are invalid → need correction.\n",
    "________________________________________\n",
    "5. Precipitation (mm/min)\n",
    "Histogram Interpretation:\n",
    "•\tHighly right-skewed: most minutes have no rain (0), occasional high rainfall minutes create a long tail.\n",
    "•\tNegative values are impossible → indicate errors.\n",
    "Boxplot Interpretation:\n",
    "•\tOutliers correspond to intense rain events.\n",
    "•\tMedian is likely 0 or very low, reflecting mostly dry periods.\n",
    "________________________________________\n",
    "Summary of What to Look For\n",
    "1.\tFrom Histograms:\n",
    "o\tDistribution shape → normal, skewed, bimodal\n",
    "o\tPeaks → typical values\n",
    "o\tImpossible values (negative for radiation, wind, precipitation)\n",
    "2.\tFrom Boxplots:\n",
    "o\tOutliers → unusually high or low values\n",
    "o\tMedian & quartiles → typical operating range\n",
    "o\tFlags potential sensor errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e272b58",
   "metadata": {},
   "source": [
    "## Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbf50b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "\n",
    "# Define relevant columns for cleaning\n",
    "cols_radiation = ['GHI', 'DNI', 'DHI']\n",
    "cols_sensor = ['ModA', 'ModB']\n",
    "cols_wind = ['WS', 'WSgust']\n",
    "cols_misc = ['Cleaning', 'Precipitation']\n",
    "\n",
    "cols_numeric_for_impute = cols_radiation + cols_sensor + cols_wind + ['Precipitation']\n",
    "cols_for_zscore = cols_radiation + cols_sensor + cols_wind\n",
    "\n",
    "# ---------------------------\n",
    "#  Handle missing values: Impute median for key numeric columns\n",
    "# ---------------------------\n",
    "df_cleaned = df.copy()\n",
    "\n",
    "for col in cols_numeric_for_impute:\n",
    "    median_value = df_cleaned[col].median()\n",
    "    df_cleaned[col] = df_cleaned[col].fillna(median_value)\n",
    "\n",
    "# For Cleaning, fill missing with 0 (assuming no cleaning event if missing)\n",
    "df_cleaned['Cleaning'] = df_cleaned['Cleaning'].fillna(0)\n",
    "\n",
    "# ---------------------------\n",
    "# Remove impossible values\n",
    "# ---------------------------\n",
    "# Negative values for radiation, wind, precipitation\n",
    "df_cleaned = df_cleaned[(df_cleaned[cols_radiation] >= 0).all(axis=1)]\n",
    "df_cleaned = df_cleaned[(df_cleaned[cols_wind] >= 0).all(axis=1)]\n",
    "df_cleaned = df_cleaned[df_cleaned['Precipitation'] >= 0]\n",
    "\n",
    "# Ensure Cleaning is only 0 or 1\n",
    "df_cleaned = df_cleaned[df_cleaned['Cleaning'].isin([0, 1])]\n",
    "\n",
    "# ---------------------------\n",
    "# Remove extreme outliers using Z-score (|Z|>3)\n",
    "# ---------------------------\n",
    "z_scores = df_cleaned[cols_for_zscore].apply(zscore)\n",
    "\n",
    "# Keep rows where all Z-scores are within ±3\n",
    "df_cleaned = df_cleaned[(np.abs(z_scores) <= 3).all(axis=1)]\n",
    "\n",
    "# ---------------------------\n",
    "# Export cleaned dataset (all columns included)\n",
    "# ---------------------------\n",
    "output_path = r\"D:\\Python\\Week_01\\Assignment\\solar-challenge-week0\\data\\sierralione_clean.csv\"\n",
    "df_cleaned.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset exported to: {output_path}\")\n",
    "print(f\"Original rows: {len(df)}, Cleaned rows: {len(df_cleaned)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0537a1",
   "metadata": {},
   "source": [
    "## Bivariate Analysis\n",
    "### Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f41196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns available: ['GHI', 'DNI', 'DHI', 'ModA', 'ModB', 'Tamb', 'RH', 'WS', 'WSgust', 'WSstdev', 'WD', 'WDstdev', 'BP', 'Cleaning', 'Precipitation', 'TModA', 'TModB', 'Comments', 'outlier_columns', 'hour']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Column 'Timestamp' not found in the dataset. Check column names.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[86]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Ensure Timestamp column exists and convert to datetime\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mTimestamp\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df.columns:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mColumn \u001b[39m\u001b[33m'\u001b[39m\u001b[33mTimestamp\u001b[39m\u001b[33m'\u001b[39m\u001b[33m not found in the dataset. Check column names.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mTimestamp\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(df[\u001b[33m'\u001b[39m\u001b[33mTimestamp\u001b[39m\u001b[33m'\u001b[39m], errors=\u001b[33m'\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Drop rows where Timestamp could not be parsed\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: \"Column 'Timestamp' not found in the dataset. Check column names.\""
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"D:\\Python\\Week_01\\data\\data\\sierraleone-bumbuna.csv\")# Strip extra spaces in column names\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Check columns\n",
    "print(\"Columns available:\", df.columns.tolist())\n",
    "\n",
    "# Ensure Timestamp column exists and convert to datetime\n",
    "if 'Timestamp' not in df.columns:\n",
    "    raise KeyError(\"Column 'Timestamp' not found in the dataset. Check column names.\")\n",
    "\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'], errors='coerce')\n",
    "\n",
    "# Drop rows where Timestamp could not be parsed\n",
    "df = df.dropna(subset=['Timestamp'])\n",
    "\n",
    "# Sort by Timestamp\n",
    "df = df.sort_values('Timestamp')\n",
    "\n",
    "# Set Timestamp as index (optional but useful for plotting)\n",
    "df.set_index('Timestamp', inplace=True)\n",
    "\n",
    "# ---------------------------\n",
    "# 2️⃣ Line chart for GHI, DNI, DHI, Tamb\n",
    "# ---------------------------\n",
    "plt.figure(figsize=(16, 6))\n",
    "for col, color in zip(['GHI', 'DNI', 'DHI', 'Tamb'], ['orange', 'red', 'green', 'blue']):\n",
    "    if col in df.columns:\n",
    "        plt.plot(df.index, df[col], label=col, color=color)\n",
    "    else:\n",
    "        print(f\"Warning: Column '{col}' not found in dataset.\")\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Time Series Line Chart: GHI, DNI, DHI, Tamb')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# 3️⃣ Optional: Bar charts (daily average)\n",
    "# ---------------------------\n",
    "# Resample to daily averages\n",
    "df_daily = df.resample('D').mean()\n",
    "\n",
    "for col in ['GHI', 'DNI', 'DHI', 'Tamb']:\n",
    "    if col in df_daily.columns:\n",
    "        plt.figure(figsize=(16, 4))\n",
    "        sns.barplot(x=df_daily.index, y=df_daily[col], color='skyblue')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel(col)\n",
    "        plt.title(f'Daily Average {col} vs. Timestamp')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bda2c8e",
   "metadata": {},
   "source": [
    "### Observe patterns by month, trends throughout the day, or anomalies, such as peaks in solar irradiance or temperature fluctuations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cb7bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
